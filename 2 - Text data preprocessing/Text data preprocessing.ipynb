{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://supaerodatascience.github.io/machine-learning/\">https://supaerodatascience.github.io/machine-learning/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Text data pre-processing</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercice, we shall load a database of email messages and pre-format them so that we can design automated classification methods or use off-the-shelf classifiers. The general purpose of this notebook is to give a practical notion (through this example) of how important data pre-processing can be in a Machine Learning workflow, and generalize it to other situations.\n",
    "\n",
    "\"What is there to pre-process?\" you might ask. Well, actually, text data comes in a very noisy form that we, humans, have become accustomed to and filter out effortlessly to grasp the core meaning of the text. It has a lot of formatting (fonts, colors, typography...), punctuation, abbreviations, common words, grammatical rules, etc. that we might wish to discard before even starting the data analysis.\n",
    "\n",
    "Here are some pre-processing steps that can be performed on text:\n",
    "1. loading the data, removing attachements, merging title and body;\n",
    "2. tokenizing - splitting the text into atomic \"words\";\n",
    "3. removal of stop-words - very common words;\n",
    "4. removal of non-words - punctuation, numbers, gibberish;\n",
    "3. lemmatization - merge together \"find\", \"finds\", \"finder\".\n",
    "\n",
    "The final goal is to be able to represent a document as a mathematical object, e.g. a vector, that our machine learning black boxes can process.\n",
    "\n",
    "在本练习中，我们将加载电子邮件消息数据库并对其进行预格式化，以便我们可以设计自动分类方法或使用现成的分类器。本笔记本的一般目的是（通过本示例）给出数据预处理在机器学习工作流程中的重要性的实用概念，并将其推广到其他情况。\n",
    "\n",
    "“有什么要预处理的？” 你可能会问。嗯，实际上，文本数据以一种非常嘈杂的形式出现，我们人类已经习惯并毫不费力地过滤掉以掌握文本的核心含义。它有很多格式（字体、颜色、排版...）、标点符号、缩写、常用词、语法规则等，我们甚至可能希望在开始数据分析之前将其丢弃。\n",
    "\n",
    "以下是一些可以对文本执行的预处理步骤：\n",
    "\n",
    "1. 加载数据，删除附件，合并标题和正文；\n",
    "2. 标记化 - 将文本拆分为原子“单词”；\n",
    "3. 去除停用词 - 非常常见的词；\n",
    "4. 去除非单词 - 标点符号、数字、胡言乱语；\n",
    "5. 词形还原 - 将“find”、“finds”、“finder”合并在一起。\n",
    "\n",
    "最终目标是能够将文档表示为我们的机器学习黑匣子可以处理的数学对象，例如向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Load-the-data\" data-toc-modified-id=\"1.-Load-the-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>1. Load the data</a></span></li><li><span><a href=\"#2.-Filtering-out-the-noise\" data-toc-modified-id=\"2.-Filtering-out-the-noise-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>2. Filtering out the noise</a></span></li><li><span><a href=\"#3.-Even-better-filtering\" data-toc-modified-id=\"3.-Even-better-filtering-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>3. Even better filtering</a></span></li><li><span><a href=\"#4.-Term-frequency-times-inverse-document-frequency\" data-toc-modified-id=\"4.-Term-frequency-times-inverse-document-frequency-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>4. Term frequency times inverse document frequency</a></span></li><li><span><a href=\"#5.-Utility-function\" data-toc-modified-id=\"5.-Utility-function-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>5. Utility function</a></span></li></ul></div>\n",
    "\n",
    "目录¶\n",
    "1   1.加载数据\n",
    "2   2. 滤除噪音\n",
    "3   3. 更好的过滤\n",
    "4   4.词频乘以逆文档频率\n",
    "5   5. 实用功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the data\n",
    "\n",
    "Let's first load the emails.\n",
    "\n",
    "# 1. 加载数据¶\n",
    "让我们首先加载电子邮件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of emails 2893\n",
      "email file: ../data/lingspam_public/bare/part1\\3-378msg5.txt\n",
      "email is a spam: False\n",
      "Subject: t\n",
      "\n",
      "hi , help ! i have to design an experiment to do with mandarin tones as part of a phonology requirement on my graduate course . there seems to be very little literature on this in the library . if anyone can think of any on-going debates on the phonology / phonetics of mandarin tones for which an experiment would be useful , please could you give me information and references . i would welcome any suggestions at all . thanks a lot , sophia wang . ( sophia @ ling . ed . ac . uk )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_switch=1\n",
    "if(data_switch==0):\n",
    "    train_dir = '../data/ling-spam/train-mails/'\n",
    "    email_path = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]\n",
    "else:\n",
    "    train_dir = '../data/lingspam_public/bare/'\n",
    "    email_path = []\n",
    "    email_label = []\n",
    "    for d in os.listdir(train_dir):\n",
    "        folder = os.path.join(train_dir,d)\n",
    "        email_path += [os.path.join(folder,f) for f in os.listdir(folder)]\n",
    "        email_label += [f[0:3]=='spm' for f in os.listdir(folder)]\n",
    "print(\"number of emails\",len(email_path))\n",
    "email_nb = 8 # try 8 for a spam example\n",
    "print(\"email file:\", email_path[email_nb])\n",
    "print(\"email is a spam:\", email_label[email_nb])\n",
    "print(open(email_path[email_nb]).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Filtering out the noise\n",
    "\n",
    "One nice thing about scikit-learn is that is has lots of preprocessing utilities. Like [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for instance, that converts a collection of text documents to a matrix of token counts.\n",
    "\n",
    "- To remove stop-words, we set: `stop_words='english'`\n",
    "- To convert all words to lowercase: `lowercase=True`\n",
    "- The default tokenizer in scikit-learn removes punctuation and only keeps words of more than 2 letters.\n",
    "\n",
    "# 2. 滤除噪音\n",
    "\n",
    "scikit-learn 的一个好处是它有很多预处理实用程序。像CountVectorizer例如，该转换的文本文档令牌计数的矩阵的集合。\n",
    "\n",
    "- 为了去除停用词，我们设置： stop_words='english'\n",
    "- 将所有单词转换为小写： lowercase=True\n",
    "- scikit-learn 中的默认标记器删除标点符号，只保留超过 2 个字母的单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvect = CountVectorizer(input='filename', stop_words='english', lowercase=True)\n",
    "word_count = countvect.fit_transform(email_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2893\n",
      "Number of words: 60618\n",
      "Document - words matrix: (2893, 60618)\n",
      "First words: ['00', '000', '0000', '00001', '00003000140', '00003003958', '00007', '0001', '00010', '00014', '0003', '00036', '000bp', '000s', '000yen', '001', '0010', '0010010034', '0011', '00133', '0014', '00170', '0019', '00198', '002', '002656', '0027', '003', '0030', '0031', '00333', '0037', '0039', '003n7', '004', '0041', '0044', '0049', '005', '0057', '006', '0067', '007', '00710', '0073', '0074', '00799', '008', '009', '00919680', '0094', '00a', '00am', '00arrival', '00b', '00coffee', '00congress', '00d', '00dinner', '00f', '00h', '00hfstahlke', '00i', '00j', '00l', '00m', '00p', '00pm', '00r', '00t', '00tea', '00the', '00uzheb', '01', '0100', '01003', '01006', '0104', '0106', '01075', '0108', '011', '0111', '0117', '0118', '01202', '01222', '01223', '01225', '01232', '01235', '01273', '013', '0131', '01334', '0135', '01364', '0139', '013953', '013a']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", word_count.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Even better filtering\n",
    "\n",
    "That's already quite ok, but this pre-processing does not perform lemmatization, the list of stop-words could be better and we could wish to remove non-english words (misspelled, with numbers, etc.).\n",
    "\n",
    "A slightly better preprocessing uses the [Natural Language Toolkit](https://www.nltk.org/https://www.nltk.org/). The one below:\n",
    "- tokenizes;\n",
    "- removes punctuation;\n",
    "- removes stop-words;\n",
    "- removes non-English and misspelled words (optional);\n",
    "- removes 1-character words;\n",
    "- removes non-alphabetical words (numbers and codes essentially).\n",
    "\n",
    "# 3. 更好的过滤¶\n",
    "这已经很好了，但是这种预处理不会执行词形还原，停用词列表可能会更好，我们可能希望删除非英语单词（拼写错误、带有数字等）。\n",
    "\n",
    "稍微好一点的预处理使用Natural Language Toolkit。下面一张：\n",
    "\n",
    "- 标记化；\n",
    "- 删除标点符号；\n",
    "- 删除停用词；\n",
    "- 删除非英语和拼写错误的单词（可选）；\n",
    "- 删除 1 个字符的单词；\n",
    "- 删除非字母单词（本质上是数字和代码）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from string import punctuation\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self, remove_non_words=True):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.words = set(words.words())\n",
    "        self.remove_non_words = remove_non_words\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove stopwords\n",
    "        word_list = [word for word in word_list if word not in self.stopwords]\n",
    "        # remove non words\n",
    "        if(self.remove_non_words):\n",
    "            word_list = [word for word in word_list if word in self.words]\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word)>1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [self.wnl.lemmatize(t) for t in word_list]\n",
    "\n",
    "countvect = CountVectorizer(input='filename',tokenizer=LemmaTokenizer(remove_non_words=True))\n",
    "word_count = countvect.fit_transform(email_path)\n",
    "feat2word = {v: k for k, v in countvect.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2893\n",
      "Number of words: 14279\n",
      "Document - words matrix: (2893, 14279)\n",
      "First words: ['aa', 'aal', 'aba', 'aback', 'abacus', 'abandon', 'abandoned', 'abandonment', 'abbas', 'abbreviation', 'abdomen', 'abduction', 'abed', 'aberrant', 'aberration', 'abide', 'abiding', 'abigail', 'ability', 'ablative', 'ablaut', 'able', 'abler', 'aboard', 'abolition', 'abord', 'aboriginal', 'aborigine', 'abound', 'abox', 'abreast', 'abridged', 'abroad', 'abrogate', 'abrook', 'abruptly', 'abscissa', 'absence', 'absent', 'absolute', 'absolutely', 'absoluteness', 'absolutist', 'absolutive', 'absolutization', 'absorbed', 'absorption', 'abstract', 'abstraction', 'abstractly', 'abstractness', 'absurd', 'absurdity', 'abu', 'abundance', 'abundant', 'abuse', 'abusive', 'abyss', 'academe', 'academic', 'academically', 'academician', 'academy', 'accelerate', 'accelerated', 'accelerative', 'accent', 'accentuate', 'accentuation', 'accept', 'acceptability', 'acceptable', 'acceptance', 'acceptation', 'accepted', 'acception', 'access', 'accessibility', 'accessible', 'accessibly', 'accidence', 'accident', 'accidental', 'accidentality', 'accidentally', 'acclaim', 'accommodate', 'accommodation', 'accompany', 'accomplish', 'accomplished', 'accomplishment', 'accord', 'accordance', 'according', 'accordingly', 'account', 'accountability', 'accountant']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", word_count.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Term frequency times inverse document frequency\n",
    "\n",
    "After this first preprocessing, each document is summarized by a vector of size \"number of words in the extracted dictionnary\". For example, the first email in the list has become:\n",
    "\n",
    "# 4. 词频乘以逆文档频率¶\n",
    "在第一次预处理之后，每个文档都由一个大小为“提取字典中的单词数”的向量汇总。例如，列表中的第一封电子邮件变成了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original email:\n",
      "Subject: re : 2 . 882 s - > np np\n",
      "\n",
      "> date : sun , 15 dec 91 02 : 25 : 02 est > from : michael < mmorse @ vm1 . yorku . ca > > subject : re : 2 . 864 queries > > wlodek zadrozny asks if there is \" anything interesting \" to be said > about the construction \" s > np np \" . . . second , > and very much related : might we consider the construction to be a form > of what has been discussed on this list of late as reduplication ? the > logical sense of \" john mcnamara the name \" is tautologous and thus , at > that level , indistinguishable from \" well , well now , what have we here ? \" . to say that ' john mcnamara the name ' is tautologous is to give support to those who say that a logic-based semantics is irrelevant to natural language . in what sense is it tautologous ? it supplies the value of an attribute followed by the attribute of which it is the value . if in fact the value of the name-attribute for the relevant entity were ' chaim shmendrik ' , ' john mcnamara the name ' would be false . no tautology , this . ( and no reduplication , either . )\n",
      "\n",
      "Bag of words representation (43 words in dict):\n",
      "{'subject': 2, 'date': 1, 'sun': 1, 'ca': 1, 'anything': 1, 'interesting': 1, 'said': 1, 'construction': 2, 'second': 1, 'much': 1, 'related': 1, 'might': 1, 'consider': 1, 'form': 1, 'list': 1, 'late': 1, 'reduplication': 2, 'logical': 1, 'sense': 2, 'name': 4, 'tautologous': 3, 'thus': 1, 'level': 1, 'indistinguishable': 1, 'well': 2, 'say': 2, 'give': 1, 'support': 1, 'logic': 1, 'based': 1, 'semantics': 1, 'irrelevant': 1, 'natural': 1, 'language': 1, 'value': 3, 'attribute': 3, 'fact': 1, 'relevant': 1, 'entity': 1, 'would': 1, 'false': 1, 'tautology': 1, 'either': 1}\n",
      "\n",
      "Vector reprensentation (43 non-zero elements):\n",
      "  (0, 12153)\t2\n",
      "  (0, 3092)\t1\n",
      "  (0, 12273)\t1\n",
      "  (0, 1702)\t1\n",
      "  (0, 616)\t1\n",
      "  (0, 6566)\t1\n",
      "  (0, 10938)\t1\n",
      "  (0, 2654)\t2\n",
      "  (0, 11143)\t1\n",
      "  (0, 8094)\t1\n",
      "  (0, 10418)\t1\n",
      "  (0, 7837)\t1\n",
      "  (0, 2615)\t1\n",
      "  (0, 4979)\t1\n",
      "  (0, 7283)\t1\n",
      "  (0, 7043)\t1\n",
      "  (0, 10313)\t2\n",
      "  (0, 7349)\t1\n",
      "  (0, 11226)\t2\n",
      "  (0, 8176)\t4\n",
      "  (0, 12565)\t3\n",
      "  (0, 12800)\t1\n",
      "  (0, 7162)\t1\n",
      "  (0, 6321)\t1\n",
      "  (0, 14001)\t2\n",
      "  (0, 11030)\t2\n",
      "  (0, 5345)\t1\n",
      "  (0, 12307)\t1\n",
      "  (0, 7348)\t1\n",
      "  (0, 1126)\t1\n",
      "  (0, 11207)\t1\n",
      "  (0, 6718)\t1\n",
      "  (0, 8215)\t1\n",
      "  (0, 7019)\t1\n",
      "  (0, 13666)\t3\n",
      "  (0, 908)\t3\n",
      "  (0, 4583)\t1\n",
      "  (0, 10436)\t1\n",
      "  (0, 4204)\t1\n",
      "  (0, 14188)\t1\n",
      "  (0, 4614)\t1\n",
      "  (0, 12566)\t1\n",
      "  (0, 3992)\t1\n"
     ]
    }
   ],
   "source": [
    "mail_number = 0\n",
    "text = open(email_path[mail_number]).read()\n",
    "print(\"Original email:\")\n",
    "print(text)\n",
    "#print(LemmaTokenizer()(text))\n",
    "#print(len(set(LemmaTokenizer()(text))))\n",
    "#print(len([feat2word[i] for i in word_count2[mail_number, :].nonzero()[1]]))\n",
    "#print(len([word_count2[mail_number, i] for i in word_count2[mail_number, :].nonzero()[1]]))\n",
    "#print(set([feat2word[i] for i in word_count2[mail_number, :].nonzero()[1]])-set(LemmaTokenizer()(text)))\n",
    "emailBagOfWords = {feat2word[i]: word_count[mail_number, i] for i in word_count[mail_number, :].nonzero()[1]}\n",
    "print(\"Bag of words representation (\", len(emailBagOfWords), \" words in dict):\", sep='')\n",
    "print(emailBagOfWords)\n",
    "print(\"\\nVector reprensentation (\", word_count[mail_number, :].nonzero()[1].shape[0], \" non-zero elements):\", sep='')\n",
    "print(word_count[mail_number, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting words is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called `tf` for Term Frequencies.\n",
    "\n",
    "Another refinement on top of `tf` is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called `tf–idf` for “Term Frequency times Inverse Document Frequency” and again, scikit-learn does the job for us with the [TfidfTransformer](scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) function.\n",
    "\n",
    "计算单词是一个好的开始，但有一个问题：较长的文档比较短的文档具有更高的平均计数值，即使它们可能谈论相同的主题。\n",
    "\n",
    "为了避免这些潜在的差异，将文档中每个单词出现的次数除以文档中的单词总数就足够了：这些新特征称为tf术语频率。\n",
    "\n",
    "另一个改进tf是降低语料库中许多文档中出现的单词的权重，因此与仅出现在语料库中较小部分的单词相比，信息量更少。\n",
    "\n",
    "这种缩减被称为tf–idf“词频乘以逆文档频率”，再次，scikit-learn 使用TfidfTransformer函数为我们完成了这项工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2893, 14279)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer().fit_transform(word_count)\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now every email in the corpus has a vector representation that filters out unrelevant tokens and retains the significant information.\n",
    "\n",
    "现在，语料库中的每封电子邮件都有一个向量表示，可以过滤掉不相关的标记并保留重要信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email 0:\n",
      "  (0, 14188)\t0.0476366389153867\n",
      "  (0, 14001)\t0.10863457040408879\n",
      "  (0, 13666)\t0.29378496199758364\n",
      "  (0, 12800)\t0.08202664607875251\n",
      "  (0, 12566)\t0.1724316642310435\n",
      "  (0, 12565)\t0.5172949926931305\n",
      "  (0, 12307)\t0.07916060019893302\n",
      "  (0, 12273)\t0.11037003020069999\n",
      "  (0, 12153)\t0.0438100675761268\n",
      "  (0, 11226)\t0.16647435384469672\n",
      "  (0, 11207)\t0.06780624737245437\n",
      "  (0, 11143)\t0.06148918450544994\n",
      "  (0, 11030)\t0.15092314041652832\n",
      "  (0, 10938)\t0.08904659377287111\n",
      "  (0, 10436)\t0.07875114839373085\n",
      "  (0, 10418)\t0.06660895299901055\n",
      "  (0, 10313)\t0.2519744396619634\n",
      "  (0, 8215)\t0.06881355283176444\n",
      "  (0, 8176)\t0.21942273948546862\n",
      "  (0, 8094)\t0.0643418459994976\n",
      "  (0, 7837)\t0.07090776914535644\n",
      "  (0, 7349)\t0.09996746952846733\n",
      "  (0, 7348)\t0.09199479564318516\n",
      "  (0, 7283)\t0.054992378193498954\n",
      "  (0, 7162)\t0.07546157020826416\n",
      "  (0, 7043)\t0.08872445511740758\n",
      "  (0, 7019)\t0.03736264352558252\n",
      "  (0, 6718)\t0.13576315389323418\n",
      "  (0, 6566)\t0.08451853830618253\n",
      "  (0, 6321)\t0.1813133911240336\n",
      "  (0, 5345)\t0.07090776914535644\n",
      "  (0, 4979)\t0.05908922075754031\n",
      "  (0, 4614)\t0.12350475115017501\n",
      "  (0, 4583)\t0.07756680408497875\n",
      "  (0, 4204)\t0.13576315389323418\n",
      "  (0, 3992)\t0.06830411048025183\n",
      "  (0, 3092)\t0.06702060333222061\n",
      "  (0, 2654)\t0.17260978179488679\n",
      "  (0, 2615)\t0.09255890494043553\n",
      "  (0, 1702)\t0.07007319072631853\n",
      "  (0, 1126)\t0.059255168776885976\n",
      "  (0, 908)\t0.4072894616797025\n",
      "  (0, 616)\t0.08532494963130849\n"
     ]
    }
   ],
   "source": [
    "print(\"email 0:\")\n",
    "print(tfidf[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Utility function\n",
    "\n",
    "Let's put all this loading process into a separate file so that we can reuse it in other experiments.\n",
    "\n",
    "# 5. 实用功能¶\n",
    "让我们将所有这些加载过程放在一个单独的文件中，以便我们可以在其他实验中重用它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_spam\n",
    "spam_data = load_spam.spam_data_loader()\n",
    "spam_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email file: ../data/lingspam_public/bare/part1\\3-378msg5.txt\n",
      "email is a spam: False\n",
      "Subject: t\n",
      "\n",
      "hi , help ! i have to design an experiment to do with mandarin tones as part of a phonology requirement on my graduate course . there seems to be very little literature on this in the library . if anyone can think of any on-going debates on the phonology / phonetics of mandarin tones for which an experiment would be useful , please could you give me information and references . i would welcome any suggestions at all . thanks a lot , sophia wang . ( sophia @ ling . ed . ac . uk )\n",
      "\n",
      "Bag of words representation (30 words in dictionary):\n",
      "{'anyone': 1, 'could': 1, 'course': 1, 'design': 1, 'experiment': 2, 'give': 1, 'going': 1, 'graduate': 1, 'help': 1, 'hi': 1, 'information': 1, 'library': 1, 'ling': 1, 'literature': 1, 'little': 1, 'lot': 1, 'mandarin': 2, 'part': 1, 'phonetics': 1, 'phonology': 2, 'please': 1, 'requirement': 1, 'sophia': 2, 'subject': 1, 'thanks': 1, 'think': 1, 'useful': 1, 'wang': 1, 'welcome': 1, 'would': 2}\n"
     ]
    }
   ],
   "source": [
    "spam_data.print_email(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
